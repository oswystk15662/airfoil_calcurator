# generate_database.py (ä¸¦åˆ—åŒ–ãƒ»é«˜é€ŸåŒ–ç‰ˆ)
import os
import glob
import time
from concurrent.futures import ProcessPoolExecutor, as_completed
import multiprocessing
from tqdm import tqdm # é€²æ—ãƒãƒ¼ç”¨

# æ—¢å­˜ã®é–¢æ•°ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from xfoil_wrapper.core import generate_polar_data

# --- è¨­å®š ---
DAT_DIR = "airfoil_data/dat_files"
CSV_DIR = "airfoil_data/csv_polars"

# è¨ˆç®—æ¡ä»¶
REYNOLDS_LIST = [10000, 15000, 20000, 30000, 50000, 75000]
AOA_START = -5.0
AOA_END = 15.0
AOA_STEP = 0.5

# --- ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°: 1ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ ---
def process_single_case(args):
    """
    ä¸¦åˆ—å‡¦ç†ãƒ¯ãƒ¼ã‚«ãƒ¼ã‹ã‚‰å‘¼ã°ã‚Œã‚‹é–¢æ•°ã€‚
    å¼•æ•°ã‚’ã‚¿ãƒ—ãƒ«ã§å—ã‘å–ã‚Šã€generate_polar_data ã‚’å®Ÿè¡Œã™ã‚‹ã€‚
    """
    airfoil_name, dat_file_path, re = args
    
    output_filename = f"{airfoil_name}_re_{re}.csv"
    output_path = os.path.join(CSV_DIR, output_filename)
    
    # ã™ã§ã«æˆåŠŸã—ãŸCSVãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯ã‚’å…¥ã‚Œã¦ã‚‚è‰¯ã„ãŒã€
    # ã“ã“ã§ã¯ã€Œå†ç”Ÿæˆã€ã‚’å„ªå…ˆã—ã¦å¸¸ã«å®Ÿè¡Œã™ã‚‹
    
    success = generate_polar_data(
        airfoil_name=airfoil_name,
        dat_file_path=dat_file_path,
        reynolds=re,
        output_csv_path=output_path,
        aoa_start=AOA_START,
        aoa_end=AOA_END,
        aoa_step=AOA_STEP
    )
    
    return airfoil_name, re, success

# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---
if __name__ == "__main__":
    print("--- ğŸ› ï¸  Step 4: Building Airfoil Database (Parallelized) ---")
    
    # 1. ç¿¼å‹ãƒªã‚¹ãƒˆã®ä½œæˆ
    dat_files = glob.glob(os.path.join(DAT_DIR, "*.dat"))
    airfoils_to_run = [
        os.path.basename(f).replace(".dat", "").lower() for f in dat_files
    ]
    
    if not airfoils_to_run:
        print(f"Error: No .dat files found in {DAT_DIR}.")
        exit()
    else:
        print(f"Found {len(airfoils_to_run)} airfoils.")

    # å‡ºåŠ›å…ˆä½œæˆ
    if not os.path.exists(CSV_DIR):
        os.makedirs(CSV_DIR)

    # 2. ã‚¿ã‚¹ã‚¯ï¼ˆä»•äº‹ï¼‰ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    # (ç¿¼å‹, datãƒ‘ã‚¹, Reæ•°) ã®çµ„ã¿åˆã‚ã›ã‚’å…¨éƒ¨ä½œã‚‹
    tasks = []
    for airfoil_name in airfoils_to_run:
        dat_file = os.path.join(DAT_DIR, f"{airfoil_name}.dat")
        if not os.path.exists(dat_file):
            continue
            
        for re in REYNOLDS_LIST:
            tasks.append((airfoil_name, dat_file, re))

    # CPUã‚³ã‚¢æ•°ã®å–å¾— (è«–ç†ã‚³ã‚¢æ•°)
    max_workers = multiprocessing.cpu_count()
    print(f"Starting parallel execution with {max_workers} workers...")
    print(f"Total tasks: {len(tasks)}")
    
    total_start_time = time.time()
    
    # 3. ä¸¦åˆ—å®Ÿè¡Œ
    success_count = 0
    fail_count = 0
    
    # ProcessPoolExecutorã§ä¸¦åˆ—åŒ–
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        # tqdmã§é€²æ—ãƒãƒ¼ã‚’è¡¨ç¤º
        # executor.submit ã§ã‚¿ã‚¹ã‚¯ã‚’æŠ•ã’ã€as_completed ã§çµ‚ã‚ã£ãŸé †ã«å‡¦ç†
        futures = [executor.submit(process_single_case, task) for task in tasks]
        
        for future in tqdm(as_completed(futures), total=len(tasks), unit="polars"):
            airfoil, re, is_success = future.result()
            if is_success:
                success_count += 1
            else:
                fail_count += 1
                # å¤±æ•—ã—ãŸã¨ãã ã‘è©³ç´°ã‚’è¡¨ç¤ºã—ãŸã„å ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆã‚’å¤–ã™
                # print(f"\nFailed: {airfoil} @ Re={re}")

    total_time = time.time() - total_start_time
    
    print("\n------------------------------------------")
    print(f"âœ… Database generation complete in {total_time:.2f} seconds.")
    print(f"   Success: {success_count}")
    print(f"   Failed:  {fail_count}")
    print("------------------------------------------")